@article{lasso,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267-288},
 publisher = {Wiley},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}

% pathwise optimization
@article{friedman2010regularization,
  title={Regularization paths for generalized linear models via coordinate descent},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  journal={Journal of statistical software},
  volume={33},
  number={1},
  pages={1},
  year={2010},
  publisher={NIH Public Access}
}

% stoch coordinate descent
@article{shalev2011stochastic,
  title={Stochastic methods for l 1-regularized loss minimization},
  author={Shalev-Shwartz, Shai and Tewari, Ambuj},
  journal={The Journal of Machine Learning Research},
  volume={12},
  pages={1865--1892},
  year={2011},
  publisher={JMLR. org}
}


@article{Pereira2009S199,
title = "Machine learning classifiers and fMRI: A tutorial overview ",
journal = "NeuroImage ",
volume = "45",
number = "1, Supplement 1",
pages = "S199 - S209",
year = "2009",
note = "Mathematics in Brain Imaging ",
issn = "1053-8119",
doi = "http://dx.doi.org/10.1016/j.neuroimage.2008.11.007",
url = "http://www.sciencedirect.com/science/article/pii/S1053811908012263",
author = "Francisco Pereira and Tom Mitchell and Matthew Botvinick",
abstract = "Interpreting brain image experiments requires analysis of complex, multivariate data. In recent years, one analysis approach that has grown in popularity is the use of machine learning algorithms to train classifiers to decode stimuli, mental states, behaviours and other variables of interest from fMRI data and thereby show the data contain information about them. In this tutorial overview we review some of the key choices faced in using this approach as well as how to derive statistically significant results, illustrating each point from a case study. Furthermore, we show how, in addition to answering the question of ‘is there information about a variable of interest’ (pattern discrimination), classifiers can be used to tackle other classes of question, namely ‘where is the information’ (pattern localization) and ‘how is that information encoded’ (pattern characterization). "
}




